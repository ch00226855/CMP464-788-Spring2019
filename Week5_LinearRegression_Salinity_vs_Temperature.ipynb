{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Linear Models\n",
    "\n",
    "Today we are going to cover the univariate (single variable) and multivariate linear regression model in depth:\n",
    "\n",
    "- Model expression and assumptions\n",
    "- Model input, output, parameters, and hyper-parameters\n",
    "- Loss function: measure the performance of the model with given parameters\n",
    "- Train a linear model: least squares method, normal equation, (batch) gradient descent and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: CalCOFI from Kaggle.com\n",
    "\n",
    "Download bottle.csv from:\n",
    "\n",
    "https://www.kaggle.com/sohier/calcofi#bottle.csv\n",
    "\n",
    "- The CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. \n",
    "- CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature.\n",
    "- Today we are going to use this dataset to investigate the relationship between water salinity and water temperature. In particular, we would like to predict the change of salinity based on water temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datapath = '/home/liang/Dropbox/Teaching/CMP464/Spring2019/Data/CalCOFI/'\n",
    "bottle = pd.read_csv(datapath + 'bottle.csv', sep=',')\n",
    "bottle.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the dataset\n",
    "\n",
    "- show column names and data types\n",
    "- compute descriptive statistics for each variable\n",
    "- extract 'Salnty' and 'T_degC' for the first 500 rows. (The first 500 row were tested in March, 1949 near California coast.)\n",
    "- detect and remove missing values\n",
    "- draw scatter plot and compute correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names and data types\n",
    "print('columns:', bottle.columns)\n",
    "print('data types:', bottle.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics of each numerical column\n",
    "bottle.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'Salnty' and 'T_degC' for the first 500 rows.\n",
    "# The first 500 row were tested in March, 1949 near California coast\n",
    "data = bottle[['Salnty', 'T_degC']][:500]\n",
    "plt.plot(data['Salnty'], data['T_degC'], 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['T_degC'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Salnty'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Salnty'], data['T_degC'], 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficient\n",
    "The **Pearson's correlation coefficient** is the covariance of the two variables divided by the product of their standard deviations.\n",
    "\n",
    "- For a variable $X$ with data set $\\{x^{(1)},...,x^{(n)}\\}$, its **standard deviation** is calculated as \n",
    "\n",
    "$\\sigma_X = \\sum_{i=1}^n(x^{(i)} - \\bar{x})^2$,\n",
    "\n",
    "where $\\bar{x}$ is the **mean** of $X$,\n",
    "\n",
    "- For a pari of variables $(X, Y)$ with data set $\\{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\\}$, their **covariance** is calculated as\n",
    "\n",
    "$cov(X, Y) = \\sum_{i=1}^n(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})$.\n",
    "\n",
    "- The **correlation coefficient** of $X$ and $Y$ is defined as\n",
    "\n",
    "$\\rho_{X, Y} = \\frac{cov(X, Y)}{\\sigma_X\\sigma_Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. What kind of data set has standard deviation equal to 0?\n",
    "2. What kind of data set has covariance equal to 0?\n",
    "3. If the points $(x_i, y_i)$ are lying on a straight line, what is $\\rho_{X, Y}$?\n",
    "4. What data set has $\\rho_{X, Y}$ close to 0?\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Linear regression: model representation\n",
    "\n",
    "In order to figure out how to find the line that best fits the data, we need to specify some notations:\n",
    "\n",
    "n = number of training examples\n",
    "\n",
    "$x$: \"input\" variable - water temperature\n",
    "\n",
    "$y$: \"output\" variable - water salinity\n",
    "\n",
    "Pair $(x^{(i)}, y^{(i)})$ corresponds to the $i$-th data instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis of linear regression model:\n",
    "Hypothesis: the relationship between x and y can be described as a straight line.\n",
    "\n",
    "Mathematical expression:\n",
    "\n",
    "$y = m\\cdot x + b + noise$ \n",
    "\n",
    "### How to choose $m$ and $b$?\n",
    "On average, for a training example $(x,y)$, the value $mx+b$ should be close to the true value $y$.\n",
    "\n",
    "How to measure closeness of two values? The measurement:\n",
    "\n",
    "1. should be small when two values are close, large when two values are far apart.\n",
    "2. easy to compute, easy to minimize (require derivatives)\n",
    "\n",
    "One good choice: **mean square error (SME)**\n",
    "\n",
    "Mathematical expression:\n",
    "\n",
    "$\\min_{m, b}\\frac{1}{n}\\sum_{i=1}^{n}(mx^{(i)} + b - y^{(i)})^2$\n",
    "\n",
    "The expression to be minimized is called the cost function of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "x's: {1, 2, 3}\n",
    "\n",
    "y's: {1.3, 2.25, 3,75}\n",
    "\n",
    "Q: for $m=0.4$, $b=0.8$, what is the value for the cost function?\n",
    "\n",
    "Q: How to minimize the cost function?\n",
    "\n",
    "Method 1: find closed form solution.\n",
    "\n",
    "Method 2: use gradient descent\n",
    "\n",
    "Method 3: use stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method 1: Normal Equation for Linear Regression\n",
    "The vector $z=(m,b)$ can be obtained directly from x's and y's through the following equation:\n",
    "\n",
    "$z = (X^T X) ^ {-1} X^T y$.\n",
    "\n",
    "Here each row of $x$ and $y$ represents a training example. In order to reflect the constant term of linear expression, X should include a second column filled with 1.\n",
    "\n",
    "This formula is call the **normal equation** of linear regression. It generalizes to the multivariate case, where $X$ may have more than one column. Derivation of normal equation can be found here:https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method 2: (Batch) Gradient Descent\n",
    "The analytical method involves solving linear system of equations, which has complexity $O(n^3)$. When the size of data set is large (>1 million), this method is going to take a long time to find the results. Also, not every machine learning model has a formula that directly computes the best values for parameters. *Gradient Descent* is an optimization method that is suitable for most machine learning models.\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "\n",
    "repeat until converge{\n",
    "\n",
    "    parameter = paramter - learning rate * gradient\n",
    "    \n",
    "}\n",
    "\n",
    "1. Gradient is the vector of partial derivatives\n",
    "2. Parameters should be updated simultanously.\n",
    "3. Stopping criterion: set a maximum number of iteration, or stop when the different between the new cost and the previous cost is less than a threshold (e.g. $10^{-6}$)\n",
    "\n",
    "For cost function \n",
    "\n",
    "$J(m, b) = \\frac{1}{n}\\sum_{i=1}^{n}(mx^{(i)} + b - y^{(i)})^2$,\n",
    "\n",
    "its gradient is $(\\frac{\\partial J}{\\partial m}, \\frac{\\partial J}{\\partial b})$, and\n",
    "\n",
    "$\\frac{\\partial J}{\\partial m} = \\frac{1}{n}\\sum_{i=1}^{n} 2x^{(i)}(mx^{(i)} + b - y^{(i)})$,\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n} 2(mx^{(i)} + b - y^{(i)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method 3: Stochastic Gradient Descent\n",
    "When the dataset is extremely large (> 1 million samples, each with hundreds of features), even gradient descent algorithm is too costly. Researchers have found that gradient descent will become much more efficient if the gradient and the cost is approximated by that of a single randomly-chosen data example. This variation of gradient descent algorithm is called **stochastic gradient descent** (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Gradient Descent\n",
    "1. plot the change of the average cost.\n",
    "\n",
    "2. plot the line after each iteration (use dashed line to represent the initial guess)\n",
    "\n",
    "3. show the change of parameters on the contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Use each of the four following methods to train a univariate linear regression model on the *data* data frame generated at the beginning of the class:\n",
    "\n",
    "1. Use LinearRegression() from sklearn.linear_model\n",
    "2. Apply the normal equation\n",
    "3. Use batch gradient descent\n",
    "4. Use stochastic gradient descent\n",
    "\n",
    "They should all produce the same training result. Visualize the regression line along with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
